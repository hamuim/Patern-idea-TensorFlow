{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamuim/Patern-idea-TensorFlow/blob/main/HWSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS5cUszpZXnf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hamuim/Patern-idea-TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asz04HIkZg25"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"Patern-idea-TensorFlow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvbwGDBUZlyO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USKp6HmAZr-8"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    tpu = None\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vt2oDwL4ZxBN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rn10\n",
        "import ut\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w52uPfIZ1b_"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "con_x = np.concatenate((x_train, x_test), axis=0)\n",
        "con_y = np.concatenate((y_train, y_test), axis=0)\n",
        "x_train = con_x\n",
        "y_train = con_y"
      ],
      "metadata": {
        "id": "KvFT8ax6k1vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkB9OM4lZ4tl"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64 * strategy.num_replicas_in_sync\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "def scale(image, label):\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    label = tf.cast(label, tf.int32)\n",
        "    return image, label\n",
        "\n",
        "def augment(image,label):\n",
        "    image = tf.image.resize_with_crop_or_pad(image, 40, 40)\n",
        "    image = tf.image.random_crop(image, size=[32, 32, 3])\n",
        "    image = tf.image.random_brightness(image, max_delta=0.5)\n",
        "    image = tf.clip_by_value(image, 0., 1.)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = (\n",
        "    train_ds\n",
        "    .shuffle(1024)\n",
        "    .map(scale, num_parallel_calls=AUTO)\n",
        "    .map(augment, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = (\n",
        "    test_ds\n",
        "    .map(scale, num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af2ghlA1Foof"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "class HWSHModel(tf.keras.Model):\n",
        "    def __init__(self, resnet_model, rho=0.06):\n",
        "        super(HWSHModel, self).__init__()\n",
        "        self.resnet_model = resnet_model\n",
        "        self.rho = rho\n",
        "\n",
        "    def train_step(self, data):\n",
        "        (images, labels) = data\n",
        "        e_ws = []\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.resnet_model(images)\n",
        "            loss = self.compiled_loss(labels, predictions)\n",
        "        trainable_params = self.resnet_model.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_params)\n",
        "        grad_norm = self._grad_norm(gradients)\n",
        "        scale = self.rho / (grad_norm + 1e-12)\n",
        "\n",
        "        for (grad, param) in zip(gradients, trainable_params):\n",
        "            e_w = grad * scale\n",
        "            param.assign_add(e_w)\n",
        "            e_ws.append(e_w)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.resnet_model(images)\n",
        "            loss = self.compiled_loss(labels, predictions)\n",
        "\n",
        "        sam_gradients = tape.gradient(loss, trainable_params)\n",
        "        for (param, e_w) in zip(trainable_params, e_ws):\n",
        "            param.assign_sub(e_w)\n",
        "\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(sam_gradients, trainable_params))\n",
        "\n",
        "        self.compiled_metrics.update_state(labels, predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        (images, labels) = data\n",
        "        predictions = self.resnet_model(images, training=False)\n",
        "        loss = self.compiled_loss(labels, predictions)\n",
        "        self.compiled_metrics.update_state(labels, predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def _grad_norm(self, gradients):\n",
        "        norm = tf.norm(\n",
        "            tf.stack([\n",
        "                tf.norm(grad) for grad in gradients if grad is not None\n",
        "            ])\n",
        "        )\n",
        "        return norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc0giVUSaNfs"
      },
      "outputs": [],
      "source": [
        "train_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=10,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5,\n",
        "        patience=3, verbose=1\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBHeMyrJ1Cp-"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = HWSHModel(ut.get_training_model())\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(train_ds,\n",
        "                   validation_data=test_ds,\n",
        "                   callbacks=train_callbacks,\n",
        "                   epochs=35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMT-FUYuTdSy"
      },
      "outputs": [],
      "source": [
        "ut.plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pol4QnciSm0P"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = ut.get_training_model()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(train_ds,\n",
        "                   validation_data=test_ds,\n",
        "                   callbacks=train_callbacks,\n",
        "                   epochs=35)\n",
        "print(f\"Total training time: {(time.time() - start)/60.} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZJaoiDaSo5f"
      },
      "outputs": [],
      "source": [
        "ut.plot_history(history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
